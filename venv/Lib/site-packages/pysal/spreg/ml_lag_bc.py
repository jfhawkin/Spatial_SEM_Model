"""
ML Estimation of Spatial Lag Model with Box Cox transformations
"""

__author__ = "Jason Hawkins"

import numpy as np
import numpy.linalg as la
from scipy import sparse as sp
from scipy.sparse.linalg import splu as SuperLU
import pysal as ps
from .utils import RegressionPropsY, RegressionPropsVM, inverse_prod
from .sputils import spdot, spfill_diagonal, spinv, spbroadcast
from . import diagnostics as DIAG
from . import user_output as USER
from . import summary_output as SUMMARY
from .w_utils import symmetrize
try:
    from scipy.optimize import minimize
    minimize_available = True
except ImportError:
    minimize_available = False

__all__ = ["ML_Lag_BC"]


class BaseML_Lag_BC(RegressionPropsY, RegressionPropsVM):

    """
    ML estimation of the spatial lag model with Box-Cox transformations(note no consistency
    checks, diagnostics or constants added); Anselin (1988) [Anselin1988]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
                   if 'ord', Ord eigenvalue method
                   if 'LU', LU sparse matrix decomposition
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product

    Attributes
    ----------
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    rho          : float
                   estimate of spatial autoregressive coefficient
    lam          : float
                   estimate of Box-Cox transformation coefficient on dependent variable
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) continuous variable, excluding the constant
    z            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) discrete/categorical variable, excluding the constant
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
                   if 'ord' : Ord eigenvalue method
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1)
    vm1          : array
                   Variance covariance matrix (k+2 x k+2) includes sigma2
    sig2         : float
                   Sigma squared used in computations
    logll        : float
                   maximized log-likelihood (including constant terms)
    predy_e      : array
                   predicted values from reduced form
    e_pred       : array
                   prediction errors using reduced form predicted values

    """

    def __init__(self, y, x, z, w, method='full', epsilon=0.0000001, LM=False):
        # set up main regression variables and spatial filters
        self.y = y
        self.x = x
        self.z = z
        self.n = self.x.shape[0]
        self.k = self.x.shape[1] + self.z.shape[1]
        self.method = method
        self.epsilon = epsilon

        x0 = [0.0, 0.5]
        methodML = method.upper()
        # Set the initial parameter values for estimation
        # call minimizer using concentrated log-likelihood to get rho/lambda
        if methodML in ['FULL', 'LU', 'ORD']:
            if methodML == 'FULL':
                W = w.full()[0]    # moved here
                res = minimize(lag_c_loglik, x0, bounds=((-1.0, 1.0),(None, None)),
                                      args=(
                                          self.y, self.x, self.z, self.n, w, W), method='L-BFGS-B',
                                      tol=epsilon)
                self.rho = res.x[0]
                self.lam = res.x[1]

            # NOT IMPLEMENTED
            # elif methodML == 'LU':
            #     I = sp.identity(w.n)
            #     Wsp = w.sparse  # moved here
            #     W = Wsp
            #     res = minimize(lag_c_loglik_sp, x0, bounds=((-1.0, 1.0),(None, None)),
            #                           args=(self.n, e0, e1, I, Wsp),
            #                           method='L-BFGS-B', tol=epsilon)
            # elif methodML == 'ORD':
            #     # check on symmetry structure
            #     if w.asymmetry(intrinsic=False) == []:
            #         ww = symmetrize(w)
            #         WW = np.array(ww.todense())
            #         evals = la.eigvalsh(WW)
            #         W = WW
            #     else:
            #         W = w.full()[0]     # moved here
            #         evals = la.eigvals(W)
            #     res = minimize(lag_c_loglik_ord, x0, bounds=((-1.0, 1.0),(None, None)),
            #                           args=(
            #                               self.n, e0, e1, evals), method='L-BFGS-B',
            #                           tol=epsilon)
        else:
            # program will crash, need to catch
            print(("{0} is an unsupported method".format(methodML)))
            self = None
            return

        # compute full log-likelihood, including constants
        ln2pi = np.log(2.0 * np.pi)
        llik = -res.fun - self.n / 2.0 * ln2pi - self.n / 2.0
        self.logll = llik[0][0]

        # b, residuals and predicted values


        b, e0, e1, ylam, self.ybc = self.get_beta_err(self.rho, self.lam, w)

        self.betas = np.vstack((b, self.rho, self.lam))   # rho and lambda added as last coefficients
        self.u = e0 - self.rho * e1
        # Currently calculated using BC transformed values
        self.predy = self.ybc - self.u

        # predicted y in Box-Cox units
        self.predy_e, self.predy_e_lam, self.predy_e_bc = self.get_pred_y(w, b, self.rho, self.lam, epsilon)

        yty = self.predy_e_lam.T * self.predy_e_lam

        # residual variance
        self._cache = {}
        self.sig2 = self.sig2n  # no allowance for division by n-k

        # Calculate the information matrix
        self.vm1 = self.inf_mat(W, w, b, self.ybc, self.predy_e, self.predy_e_lam, self.predy_e_bc, self.rho, self.lam)  # vm1 includes variance for sigma2
        self.vm = self.vm1[:-1, :-1]  # vm is for coefficients only
        # CHECK
        self.vm[-1, -1] = np.abs(self.vm[-1, -1])

        if LM:
            x07 = [res.x[1]]
            x08 = [res.x[0]]
            x09 = [res.x[0]]
            # Get the betas
            bb1, e0, e1, ylam1, ybc1 = self.get_beta_err(0, 0, w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f1 = (spdot(a, ybc1) - spdot(self.xfull, bb1)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 1
            F1, K1 = self.grad_vect(W, bb1, ybc1, 0, 1, 1)

            # Get the betas
            bb2, e0, e1, ylam2, ybc2 = self.get_beta_err(0, 1, w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f2 = (spdot(a, ybc2) - spdot(self.xfull, bb2)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 2
            F2, K2 = self.grad_vect(W, bb2, ybc2, 0, 1, 2)

            # Get the betas
            bb3, e0, e1, ylam3, ybc3 = self.get_beta_err(0, 0, w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f3 = (spdot(a, ybc3) - spdot(self.xfull, bb3)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 3
            F3, K3 = self.grad_vect(W, bb3, ybc3, 0, 0, 3)

            # Get the betas
            bb4, e0, e1, ylam4, ybc4 = self.get_beta_err(0, 1, w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f4 = (spdot(a, ybc4) - spdot(self.xfull, bb4)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 4
            F4, K4 = self.grad_vect(W, bb4, ybc4, 0, 1, 4)

            # Get the betas
            bb5, e0, e1, ylam5, ybc5 = self.get_beta_err(0, 0, w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f5 = (spdot(a, ybc5) - spdot(self.xfull, bb5)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 5
            F5, K5 = self.grad_vect(W, bb5, ybc5, 0, 0, 5)

            # Get the betas
            bb6, e0, e1, ylam6, ybc6 = self.get_beta_err(0, 1, w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f6 = (spdot(a, ybc6) - spdot(self.xfull, bb6)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 6
            F6, K6 = self.grad_vect(W, bb6, ybc6, 0, 1, 6)

            res7 = minimize(lag_c_loglik, x07, bounds=((None, None),),
                           args=(
                               self.y, self.x, self.z, self.n, w, W, 7), method='L-BFGS-B',
                           tol=epsilon)
            # Get the betas
            bb7, e0, e1, ylam7, ybc7 = self.get_beta_err(0, res7.x[0], w)
            a = 0 * W
            spfill_diagonal(a, 1.0)
            f7 = (spdot(a, ybc7) - spdot(self.xfull, bb7)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 7
            F7, K7 = self.grad_vect(W, bb7, ybc7, 0, res7.x[0], 7)

            res8 = minimize(lag_c_loglik, x08, bounds=((-1.0, 1.0),),
                           args=(
                               self.y, self.x, self.z, self.n, w, W, 8), method='L-BFGS-B',
                           tol=epsilon)
            # Get the betas
            bb8, e0, e1, ylam8, ybc8 = self.get_beta_err(res8.x[0], 0, w)
            a = res8.x[0] * W
            spfill_diagonal(a, 1.0)
            f8 = (spdot(a, ybc8) - spdot(self.xfull, bb8)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 8
            F8, K8 = self.grad_vect(W, bb8, ybc8, res8.x[0], 0, 8)

            res9 = minimize(lag_c_loglik, x09, bounds=((-1.0, 1.0),),
                           args=(
                               self.y, self.x, self.z, self.n, w, W, 9), method='L-BFGS-B',
                           tol=epsilon)
            # Get the betas
            bb9, e0, e1, ylam9, ybc9 = self.get_beta_err(res9.x[0], 1, w)
            a = res9.x[0] * W
            spfill_diagonal(a, 1.0)
            f9 = (spdot(a, ybc9) - spdot(self.xfull, bb9)) / np.sqrt(self.sig2)
            # Get the gradient vector for LM test 9
            F9, K9 = self.grad_vect(W, bb9, ybc9, res9.x[0], 1, 9)

            self.LM_J_R0_L0 = get_LM_test(f1, F1, K1)
            self.LM_J_R0_L1 = get_LM_test(f2, F2, K2)
            self.LM_1_R0_L0 = get_LM_test(f3, F3, K3)
            self.LM_1_R0_L1 = get_LM_test(f4, F4, K4)
            self.LM_1_L0_R0 = get_LM_test(f5, F5, K5)
            self.LM_1_L1_R0 = get_LM_test(f6, F6, K6)
            self.LM_C_R0_LU = get_LM_test(f7, F7, K7)
            self.LM_C_RU_L0 = get_LM_test(f8, F8, K8)
            self.LM_C_RU_L1 = get_LM_test(f9, F9, K9)

    def get_beta_err(self, rho, lam, w):
        ylam = self.y**lam
        if lam == 0:
            ybc = np.log(self.y)
            xbc = np.log(self.x)
        else:
            ybc = (self.y**lam - 1) / lam
            xbc = (self.x ** lam - 1) / lam

        ylag = ps.lag_spatial(w, ybc)
        # b0, b1, e0 and e1
        self.xfull = np.hstack((xbc, self.z))
        xtx = spdot(self.xfull.T, self.xfull)
        xtxi = la.inv(xtx)
        xty = spdot(self.xfull.T, ybc)
        xtyl = spdot(self.xfull.T, ylag)
        b0 = np.dot(xtxi, xty)
        b1 = np.dot(xtxi, xtyl)
        b = b0 - rho * b1
        e0 = ybc - spdot(self.xfull, b0)
        e1 = ylag - spdot(self.xfull, b1)
        return b, e0, e1, ylam, ybc

    def get_pred_y(self, w, b, rho, lam, epsilon):
        xb = spdot(self.xfull, b)
        ybc = inverse_prod(
            w.sparse, xb, rho, inv_method="power_exp", threshold=epsilon)
        ybc[ybc < 0] = 0.0001
        # convert Box-Cox y into standard y with lambda applied
        if lam == 0:
            ylam = np.exp(ybc)
            ylam[ylam < 0] = 0.0001
            # convert standard y with lambda into standard y values
            y = ylam
        else:
            ylam = ybc * lam + 1
            ylam[ylam < 0] = 0.0001
            # convert standard y with lambda into standard y values
            y = ylam ** (1 / lam)

        # Distances must be greater than 0 (and values in general for the logarithm to be taken)
        # CODE DOES NOT WORK FOR NON POSITIVE DEPENDENT VARIABLES
        y[y<0] = 0.0001

        return y, ylam, ybc

    def grad_vect(self, W, b, ybc, rho, lam, case=0):
        # gradient vector
        xb = spdot(self.xfull, b)

        # Number of Box-Cox independent (continuous) variables
        xsize = self.x.shape[1]
        # List of non-Box-Cox indices
        idx = [0] + [i for i in range(xsize+1,self.k)]

        a = -rho * W
        spfill_diagonal(a, 1.0)
        ai = spinv(a)

        lny = np.log(self.y)
        lnx = np.log(self.x)
        # Build components of lambda var/covar
        # d2F/dlam2
        if lam == 0:
            # c(y, 0)
            cy = lny**2 / 2
            cx = lnx ** 2 / 2
        elif lam == 1:
            # c(y, 1)
            cy = self.y * lny - self.y + 1
            cx = self.x * lnx - self.x + 1
        else:
            cy = (lny * lam * self.y**lam - self.y**lam + 1) / (lam ** 2)
            cx = (lnx * lam * self.x - self.x**lam + 1) / (lam ** 2)

        xbc = (self.x ** lam - 1) / lam

        # Define 1st order derivatives
        # Joint test of rho = 0 and lam = 0
        if case == 1:
            f1 = -(lny - spdot(lnx, b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -lnx / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, lny) / np.sqrt(self.sig2)
            f5 = (lny**2 - spdot(lnx**2, b[1:xsize+1])) / (2 * np.sqrt(self.sig2))

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1,1))
            k2 = np.zeros((self.n, self.k))
            k3 = -la.eigvalsh(W).reshape((-1, 1))
            k4 = lny.reshape((-1,1))
        # Joint test of rho = 0 and lam = 1
        elif case == 2:
            f1 = -((self.y - 1) - spdot((self.x - 1), b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -(self.x - 1) / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, (self.y - 1)) / np.sqrt(self.sig2)
            xHlnx = self.x * lnx - self.x + 1
            f5 =  (self.y * lny - self.y + 1 - spdot(xHlnx, b[1:xsize+1]))/ np.sqrt(self.sig2)

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = -la.eigvalsh(W).reshape((-1, 1))
            k4 = lny.reshape((-1, 1))
        # One-directional test of rho = 0, given lam = 0
        elif case == 3:
            f1 = -(lny - spdot(lnx, b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -lnx / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, lny) / np.sqrt(self.sig2)

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = -la.eigvalsh(W).reshape((-1, 1))
        # One-directional test of rho = 0, given lam = 1
        elif case == 4:
            f1 = -((self.y - 1) - spdot((self.x - 1), b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -(self.x - 1) / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, (self.y - 1)) / np.sqrt(self.sig2)

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = -la.eigvalsh(W).reshape((-1, 1))
        # One-directional test of lam = 0, given rho = 0
        elif case == 5:
            f1 = -(lny - spdot(lnx, b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -lnx / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = (lny**2 - spdot(lnx**2, b[1:xsize+1])) / (2 * np.sqrt(self.sig2))

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = lny.reshape((-1, 1))
        # One-directional test of lam = 1, given rho = 0
        elif case == 6:
            f1 = -((self.y - 1) - spdot((self.x - 1), b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -(self.x - 1) / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            yHlny = self.y * lny - self.y + 1
            xHlnx = self.x * lnx - self.x + 1
            f4 = (yHlny - spdot(xHlnx, b[1:xsize+1])) / np.sqrt(self.sig2)

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = lny.reshape((-1, 1))

        # Conditional test of rho = 0 and lam estimated
        elif case == 7:
            f1 = -(ybc - spdot(xbc, b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -xbc / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, lny) / np.sqrt(self.sig2)
            f5 = (cy - cx) / np.sqrt(self.sig2)

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = -la.eigvalsh(W).reshape((-1, 1))
            k4 = lny.reshape((-1, 1))
        # Conditional test of lam = 0 and rho estimated
        elif case == 8:
            f1 = -(spdot(a, lny) - spdot(lnx, b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -lnx / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, lny) / np.sqrt(self.sig2)
            f5 = (spdot(a, lny**2) - spdot(lnx**2, b[1:xsize+1])) / (2 * np.sqrt(self.sig2))

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            k3 = -spdot(la.eigvalsh(W), ai).reshape((-1, 1))
            k4 = lny.reshape((-1, 1))
        # Conditional test of lam = 1 and rho estimated
        elif case == 9:
            f1 = -(spdot(a, (self.y - 1)) - spdot((self.x - 1), b[1:xsize+1]) - spdot(self.z, b[idx])) / self.sig2
            f2 = -(self.x - 1) / np.sqrt(self.sig2)
            f3 = -self.z / np.sqrt(self.sig2)
            f4 = -spdot(W, (self.y - 1)) / np.sqrt(self.sig2)
            yHlny = self.y * lny - self.y + 1
            xHlnx = self.x * lnx - self.x + 1
            f5 = (spdot(a, yHlny) - spdot(xHlnx, b[1:xsize+1])) / np.sqrt(self.sig2)

            k1 = np.repeat(-1 / np.sqrt(self.sig2), self.n).reshape((-1, 1))
            k2 = np.zeros((self.n, self.k))
            ai = spinv(a)
            k3 = -spdot(la.eigvalsh(W), ai).reshape((-1, 1))
            k4 = lny.reshape((-1, 1))
        else:
            return print("Case not defined.")
        if case in [1,2,7,8,9]:
            F = np.hstack((f1, f2, f3, f4, f5))
            K = np.hstack((k1, k2, k3, k4))
        else:
            F = np.hstack((f1, f2, f3, f4))
            K = np.hstack((k1, k2, k3))

        return F, K

    def inf_mat(self, W, w, b, ybc, pred_y, pred_ylam, pred_ybc, rho, lam):
        # information matrix
        xb = spdot(self.xfull, b)
        y = self.y
        # Number of Box-Cox independent (continuous) variables
        xsize = self.x.shape[1]

        xtx = spdot(self.xfull.T, self.xfull)
        a = -rho * W
        spfill_diagonal(a, 1.0)
        ai = spinv(a)
        wai = spdot(W, ai)
        tr1 = wai.diagonal().sum()  # same for sparse and dense

        wai2 = np.dot(wai, wai)
        tr2 = wai2.diagonal().sum()

        waiTwai = np.dot(wai.T, wai)
        tr3 = waiTwai.diagonal().sum()

        ### From Anselin, so y is actually y_pred
        wpredy = ps.lag_spatial(w, pred_ybc)
        wpyTwpy = np.dot(wpredy.T, wpredy)
        xTwpy = spdot(self.xfull.T, wpredy)
        ### End from Anselin (y is now observed value)

        # define the fixed values used in the calculation of var/covar for lambda
        # fixed wrt lambda
        wtw = spdot(W.T, W)
        wx = spdot(W, self.xfull)
        wxb = spdot(wx, b)

        lny = np.log(self.y)
        lnx = np.log(self.x)
        # Build components of lambda var/covar
        # d2F/dlam2
        if lam == 0:
            # c(y, 0)
            cy = lny**2 / 2
            dcydl = lny**3 / 3
            cx = lnx ** 2 / 2
            dcxdl = lnx ** 3 / 3
        elif lam == 1:
            # c(y, 1)
            cy = self.y * lny - self.y + 1
            dcydl = self.y * lny**2 - 2 * self.y * lny + 2 * lny - 2
            cx = self.y * lny - self.y + 1
            dcxdl = self.y * lny ** 2 - 2 * self.y * lny + 2 * lny - 2
        else:
            cy = (lny * lam * self.y**lam - self.y**lam + 1) / (lam ** 2)
            dcydl = (lam**2 * lny ** 2 * self.y**lam + 2 * self.y**lam - 2 * lam * lny * self.y**lam - 2) \
                   / (lam**3)
            cx = (lnx * lam * self.x**lam - self.x**lam + 1) / (lam ** 2)
            dcxdl = (lam**2 * lnx ** 2 * self.x**lam + 2 * self.x**lam - 2 * lam * lnx * self.x**lam - 2) \
                   / (lam**3)

        acy = spdot(a, cy)
        cxb = spdot(cx, b[1:xsize+1])
        ac = acy - cx
        adcy = spdot(a, dcydl)
        adc = adcy - dcxdl
        xa = spdot(self.xfull.T, a)
        xac = spdot(self.xfull.T, ac)
        e = spdot(a, ybc) - xb
        wcy = spdot(W, cy)
        wly = spdot(W, lny)
        wy = spdot(ybc.T, W.T)
        p2 = y * lny**2 - 2 * self.y * lny + 2 * self.y - 2

        # Define 2nd order derivatives
        cx1 = np.zeros((1, self.k))
        cx1[0, 1:self.x.shape[1]+1] = spdot(cx.T, e) / self.sig2
        dFdlb = -xac.T / self.sig2 - cx1
        if rho == 0 and lam == 1:
            dFdll = spdot((self.y * lny - self.y + 1).T, (self.y * lny - self.y + 1)) / self.sig2 \
                    + spdot(e.T, p2) / self.sig2
        else:
            dFdll = (spdot(ac.T, ac) + spdot(e.T, adc)) / self.sig2
        dFdlr = -(spdot(wy, ac) - spdot(e.T, wcy)) / self.sig2
        dFdls = -spdot(e.T, ac) / self.sig2 ** 2

        # order of variables is beta, rho, lambda, sigma2
        v1 = np.vstack((xtx / self.sig2, xTwpy.T / self.sig2, dFdlb, np.zeros((1, self.k))))
        v2 = np.vstack((xTwpy / self.sig2, tr2 + tr3 + wpyTwpy / self.sig2, dFdlr, tr1 / self.sig2))
        v3 = np.vstack((dFdlb.T, dFdlr, dFdll, dFdls))
        v4 = np.vstack((np.zeros((self.k, 1)), tr1 / self.sig2, dFdls, self.n / (2.0 * self.sig2 ** 2)))
        v = np.hstack((v1, v2, v3, v4))

        return la.inv(v)

class ML_Lag_BC(BaseML_Lag_BC):

    """
    ML estimation of the spatial lag model with Box-Cox transformation of dependent variable
    with all results and diagnostics; Anselin (1988) [Anselin1988]

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) continuous variable, excluding the constant
    z            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) discrete/categorical variable, excluding the constant
    w            : pysal W object
                   Spatial weights object
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
                   if 'ord', Ord eigenvalue method
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product
    spat_diag    : boolean
                   if True, include spatial diagnostics
    vm           : boolean
                   if True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    rho          : float
                   estimate of spatial autoregressive coefficient
    lam          : float
                   estimate of Box-Cox transformation coefficient on dependent variable
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1), all coefficients
    vm1          : array
                   Variance covariance matrix (k+2 x k+2), includes sig2
    sig2         : float
                   Sigma squared used in computations
    logll        : float
                   maximized log-likelihood (including constant terms)
    aic          : float
                   Akaike information criterion
    schwarz      : float
                   Schwarz criterion
    LM_J_R0_L0    : float
                   Lagrange multiplier criterion for joint condition
                   of no spatial lag and log distance transformation
    LM_J_R0_L1    : float
                   Lagrange multiplier criterion for joint condition
                   of no spatial lag and linear distance transformation
    LM_C_R0_LU    : float
                   Lagrange multiplier criterion for conditional condition
                   of no spatial lag and unknown distance transformation
    LM_C_RU_L0    : float
                   Lagrange multiplier criterion for conditional condition
                   of unkown spatial lag and log distance transformation
    LM_C_RU_L1    : float
                   Lagrange multiplier criterion for conditional condition
                   of unkown spatial lag and linear distance transformation
    predy_e      : array
                   predicted values from reduced form
    e_pred       : array
                   prediction errors using reduced form predicted values
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
    utu          : float
                   Sum of squared residuals
    std_err      : array
                   1xk array of standard errors of the betas
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used

    """

    def __init__(self, y, x, z, w, method='full', epsilon=0.0000001,
                 spat_diag=False, vm=False, name_y=None, name_x=None,
                 name_z1=None, name_w=None, name_ds=None, LM=False):
        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        z_constant = USER.check_constant(z)
        method = method.upper()
        BaseML_Lag_BC.__init__(
            self, y=y, x=x, z=z_constant, w=w, method=method, epsilon=epsilon, LM=LM)
        # increase by 1 to have correct aic and sc, include rho and lambda in count
        self.k += 2
        self.title = "MAXIMUM LIKELIHOOD SPATIAL LAG WITH BOX-COX" + \
            " (METHOD = " + method + ")"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_z1 = USER.set_name_x(name_z1, z)
        name_ylag = USER.set_name_yend_sp(self.name_y)
        name_ybc = "LAMBDA"
        # combine the x and z vectors for printing
        self.name_x.extend(name_z1)
        self.name_x.append(name_ylag)  # rho changed to last position
        self.name_x.append(name_ybc)
        self.name_w = USER.set_name_w(name_w, w)
        self.aic = DIAG.akaike(reg=self)
        self.schwarz = DIAG.schwarz(reg=self)
        SUMMARY.ML_Lag_BC(reg=self, w=w, vm=vm, spat_diag=spat_diag, LM=LM)

def lag_c_loglik(beta, y, x, z, n, wps, W, case=0):
    # concentrated log-lik for lag model, no constants, brute force

    if case == 7:
        rho = 0
        lam = beta[0]
    elif case == 8:
        rho = beta[0]
        lam = 0
    elif case == 9:
        rho = beta[0]
        lam = 1
    else:
        rho = beta[0]
        lam = beta[1]

    if lam == 0:
        ybc = np.log(y)
    else:
        ybc = (y**lam - 1) / lam

    xbc = (x ** lam - 1) / lam
    xfull = np.hstack((xbc, z))
    xtx = spdot(xfull.T, xfull)
    xtxi = la.inv(xtx)

    ylag = ps.lag_spatial(wps, ybc)
    xty = spdot(xfull.T, ybc)
    xtyl = spdot(xfull.T, ylag)
    b0 = np.dot(xtxi, xty)
    b1 = np.dot(xtxi, xtyl)
    e0 = ybc - spdot(xfull, b0)
    e1 = ylag - spdot(xfull, b1)
    er = e0 - rho * e1
    sig2 = np.dot(er.T, er) / n
    nlsig2 = (n / 2.0) * np.log(sig2)
    a = -rho * W
    spfill_diagonal(a, 1.0)
    jacob = np.log(np.linalg.det(a)) + (lam - 1) * np.sum(np.log(y))
    # this is the negative of the concentrated log lik for minimization
    clik = nlsig2 - jacob
    return clik

def get_LM_test(f, F, K):
    fFK = -spdot(f.T, F) + spdot(np.ones((1, K.shape[0])), K)
    FfK = -spdot(F.T, f) + spdot(K.T, np.ones((K.shape[0],1)))
    FtF = spdot(F.T, F)
    KtK = spdot(K.T, K)
    FtFKtKi = la.inv(FtF + KtK)
    p1 = spdot(fFK, FtFKtKi)
    return spdot(p1, FfK)

def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

